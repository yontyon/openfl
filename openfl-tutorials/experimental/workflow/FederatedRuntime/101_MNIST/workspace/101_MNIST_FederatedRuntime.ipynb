{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "dc13070c",
   "metadata": {},
   "source": [
    "# FederatedRuntime 101: Quickstart with MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbe52a4e",
   "metadata": {},
   "source": [
    "Welcome to the first **FederatedRuntime** Tutorial ! \n",
    "This tutorial demonstrates how to deploy Federated-Learning experiment based on workflow interface on a distributed computing infrastructure.\n",
    "\n",
    "Data scientists often start by developing and fine-tuning Federated machine-learning models in a local environment before transitioning to a Federated setup. OpenFL supports this methodology and the Tutorial guides the user through the following steps:\n",
    "- **Simulate** a Federated Learning experiment locally using `LocalRuntime` \n",
    "- **Deploy** this experiment on Federated Infrastructure using `FederatedRuntime` from from a familiar Jupyter notebook environment\n",
    "\n",
    "**Key Features covered**:  \n",
    "1. **Simulate** Federated Learning experiment using `LocalRuntime`. Explore [101 MNIST](https://github.com/securefederatedai/openfl/blob/develop/openfl-tutorials/experimental/workflow/101_MNIST.ipynb) for insights\n",
    "2. Enable creation of workspace content by annotating Jupyter notebook with export directives. Explore [1001 Workspace Creation from JupyterNotebook](https://github.com/securefederatedai/openfl/blob/develop/openfl-tutorials/experimental/workflow/1001_Workspace_Creation_from_JupyterNotebook.ipynb) for insights\n",
    "3. **Deploy** the experiment on Federated infrastructure (Director and Envoy nodes) using `FederatedRuntime`\n",
    "\n",
    "Let's get started !\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3b0701e",
   "metadata": {},
   "source": [
    "### Getting Started"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b62ffd86",
   "metadata": {},
   "source": [
    "We begin by specifying the module where cells marked with the `#| export` directive will be automatically exported. The export directive is used to identify specific code cells in the Jupyter notebook that should be included in the generated python module. This python module is required to distribute the FL experiment.\n",
    "\n",
    "The `#| default_exp` experiment directive in the following cell sets the name of the python module as `experiment`. This name can be customized according to the user’s requirements and preferences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d79eacbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5860947",
   "metadata": {},
   "source": [
    "Once we have specified the name of the module, subsequent cells of the notebook need to be *appended* by the `#| export` directive as shown below. User should ensure that *all* the notebook functionality required in the Federated Learning experiment is included in this directive"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d109332c",
   "metadata": {},
   "source": [
    "### Installing Pre-requisties\n",
    "We start by installing OpenFL and dependencies of the workflow interface. These dependencies are exported and become requirements for the Federated Learning Environment "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7475cba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "!pip install git+https://github.com/securefederatedai/openfl.git\n",
    "!pip install -r ../../../workflow_interface_requirements.txt\n",
    "!pip install torch==2.3.1\n",
    "!pip install torchvision==0.18.1\n",
    "!pip install -U ipywidgets\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a85485b8",
   "metadata": {},
   "source": [
    "### Model definition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cc94801",
   "metadata": {},
   "source": [
    "We begin with the quintessential example of a pytorch CNN model trained on the MNIST dataset. Let's start by defining\n",
    "- Hyperparameters\n",
    "- Model definition, and \n",
    "- Helper functions to train and validate the model like we would for any other deep learning experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bd8ac2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# Hyperparameters\n",
    "learning_rate = 0.01\n",
    "momentum = 0.5\n",
    "batch_size = 32\n",
    "log_interval = 10\n",
    "\n",
    "# Model definition\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
    "        self.fc1 = nn.Linear(320, 50)\n",
    "        self.fc2 = nn.Linear(50, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
    "        x = F.relu(F.max_pool2d(self.conv2(x), 2))\n",
    "        x = x.view(-1, 320)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x)\n",
    "\n",
    "\n",
    "# Helper function to validate the model\n",
    "def validate(model, test_loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            output = model(data)\n",
    "            pred = output.data.max(1, keepdim=True)[1]\n",
    "            correct += pred.eq(target.data.view_as(pred)).sum()\n",
    "    accuracy = float(correct / len(test_loader.dataset))\n",
    "    return accuracy\n",
    "\n",
    "\n",
    "# Helper function to train the model\n",
    "def train_model(model, optimizer, data_loader, round_number, log=False):\n",
    "    train_loss = 0\n",
    "    model.train()\n",
    "    for batch_idx, (X, y) in enumerate(data_loader):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        output = model(X)\n",
    "        loss = F.nll_loss(output, y)\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item() * len(X)\n",
    "        if batch_idx % log_interval == 0 and log:\n",
    "            print(\n",
    "                \"Train Epoch: {:3} [{:5}/{:<5} ({:<.0f}%)] Loss: {:<.4f}\".format(\n",
    "                    round_number,\n",
    "                    batch_idx * len(X),\n",
    "                    len(data_loader.dataset),\n",
    "                    100.0 * batch_idx / len(data_loader),\n",
    "                    loss.item(),\n",
    "                )\n",
    "            )\n",
    "\n",
    "    train_loss /= len(data_loader.dataset)\n",
    "    return train_loss\n",
    "\n",
    "\n",
    "# Helper function to initialize seed for reproducibility\n",
    "def initialize_seed(random_seed=42):\n",
    "    torch.manual_seed(random_seed)\n",
    "    np.random.seed(random_seed)\n",
    "    random.seed(random_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "475aa38c",
   "metadata": {},
   "source": [
    "### Dataset definition\n",
    "\n",
    "We now download the training and test datasets of MNIST, a necessary step to demonstrate the functionality of the LocalRuntime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9836c542",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "import torchvision\n",
    "\n",
    "# Train and Test datasets\n",
    "mnist_train = torchvision.datasets.MNIST(\n",
    "    \"../files/\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=torchvision.transforms.Compose(\n",
    "        [\n",
    "            torchvision.transforms.ToTensor(),\n",
    "            torchvision.transforms.Normalize((0.1307,), (0.3081,)),\n",
    "        ]\n",
    "    ),\n",
    ")\n",
    "\n",
    "mnist_test = torchvision.datasets.MNIST(\n",
    "    \"../files/\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=torchvision.transforms.Compose(\n",
    "        [\n",
    "            torchvision.transforms.ToTensor(),\n",
    "            torchvision.transforms.Normalize((0.1307,), (0.3081,)),\n",
    "        ]\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4770fe7c",
   "metadata": {},
   "source": [
    "### Workflow definition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6306f73d",
   "metadata": {},
   "source": [
    "Next we import the `FLSpec`, placement decorators (`aggregator/collaborator`), and define the `FedAvg` helper function\n",
    "\n",
    "- `FLSpec` – Defines the flow specification. User defined flows are subclasses of this.\n",
    "- `aggregator/collaborator` - placement decorators that define where the task will be assigned\n",
    "- `FedAvg` - helper function for Federated Averaging\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89cf4866",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "from copy import deepcopy\n",
    "\n",
    "from openfl.experimental.workflow.interface import FLSpec\n",
    "from openfl.experimental.workflow.placement import aggregator, collaborator\n",
    "\n",
    "\n",
    "# Helper function for federated averaging\n",
    "def FedAvg(agg_model, models, weights=None):\n",
    "    state_dicts = [model.state_dict() for model in models]\n",
    "    agg_state_dict = agg_model.state_dict()\n",
    "    for key in models[0].state_dict():\n",
    "        agg_state_dict[key] = torch.from_numpy(\n",
    "            np.average([state[key].numpy() for state in state_dicts], axis=0, weights=weights)\n",
    "        )\n",
    "\n",
    "    agg_model.load_state_dict(agg_state_dict)\n",
    "    return agg_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a9d8a60",
   "metadata": {},
   "source": [
    "Let us now define the Workflow. Here we use the same tasks as the [101 MNIST](https://github.com/securefederatedai/openfl/blob/develop/openfl-tutorials/experimental/workflow/101_MNIST.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52c4a752",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "class FederatedFlow_TorchMNIST(FLSpec):\n",
    "    \"\"\"\n",
    "    This Flow trains a CNN on MNIST Model in Federated Learning\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model=None, optimizer=None, learning_rate=1e-2, momentum=0.5, rounds=3, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "        if model is not None:\n",
    "            self.model = model\n",
    "            self.optimizer = optimizer\n",
    "        else:\n",
    "            initialize_seed()\n",
    "            self.model = Net()\n",
    "            self.optimizer = optim.SGD(self.model.parameters(), lr=learning_rate, momentum=momentum)\n",
    "\n",
    "        self.learning_rate = learning_rate\n",
    "        self.momentum = momentum\n",
    "        self.rounds = rounds\n",
    "        self.results = []\n",
    "\n",
    "    @aggregator\n",
    "    def start(self):\n",
    "        \"\"\"\n",
    "        This is the start of the Flow.\n",
    "        \"\"\"\n",
    "        print(f\"Initializing Workflow .... \")\n",
    "\n",
    "        self.collaborators = self.runtime.collaborators\n",
    "        self.current_round = 0\n",
    "\n",
    "        self.next(self.aggregated_model_validation, foreach=\"collaborators\")\n",
    "\n",
    "    @collaborator\n",
    "    def aggregated_model_validation(self):\n",
    "        \"\"\"\n",
    "        Perform validation of aggregated model on collaborators.\n",
    "        \"\"\"\n",
    "        print(f\"<Collab: {self.input}> Performing Validation on aggregated model ... \")\n",
    "        self.agg_validation_score = validate(self.model, self.test_loader)\n",
    "        print(\n",
    "            f\"<Collab: {self.input}> Aggregated Model validation score = {self.agg_validation_score:.4f}\"\n",
    "        )\n",
    "\n",
    "        self.next(self.train)\n",
    "\n",
    "    @collaborator\n",
    "    def train(self):\n",
    "        \"\"\"\n",
    "        Train model on Local collaborator dataset.\n",
    "        \"\"\"\n",
    "        print(f\"<Collab: {self.input}>: Training Model on local dataset ... \")\n",
    "\n",
    "        self.optimizer = optim.SGD(self.model.parameters(), lr=self.learning_rate, momentum=self.momentum)\n",
    "\n",
    "        self.loss = train_model(\n",
    "            model=self.model,\n",
    "            optimizer=self.optimizer,\n",
    "            data_loader=self.train_loader,\n",
    "            round_number=self.current_round,\n",
    "            log=True,\n",
    "        )\n",
    "\n",
    "        self.next(self.local_model_validation)\n",
    "\n",
    "    @collaborator\n",
    "    def local_model_validation(self):\n",
    "        \"\"\"\n",
    "        Validate locally trained model.\n",
    "        \"\"\"\n",
    "        print(f\"<Collab: {self.input}> Performing Validation on locally trained model ... \")\n",
    "        self.local_validation_score = validate(self.model, self.test_loader)\n",
    "        print(\n",
    "            f\"<Collab: {self.input}> Local model validation score = {self.local_validation_score:.4f}\"\n",
    "        )\n",
    "        self.next(self.join)\n",
    "\n",
    "    @aggregator\n",
    "    def join(self, inputs):\n",
    "        \"\"\"\n",
    "        Model aggregation step.\n",
    "        \"\"\"\n",
    "        print(f\"<Agg>: Joining models from collaborators...\")\n",
    "\n",
    "        # Average Training loss, aggregated and locally trained model accuracy\n",
    "        self.average_loss = sum(input.loss for input in inputs) / len(inputs)\n",
    "        self.aggregated_model_accuracy = sum(input.agg_validation_score for input in inputs) / len(inputs)\n",
    "        self.local_model_accuracy = sum(input.local_validation_score for input in inputs) / len(inputs)\n",
    "\n",
    "        print(f\"Avg. aggregated model validation score = {self.aggregated_model_accuracy:.4f}\")\n",
    "        print(f\"Avg. training loss = {self.average_loss:.4f}\")\n",
    "        print(f\"Avg. local model validation score = {self.local_model_accuracy:.4f}\")\n",
    "\n",
    "        # FedAvg\n",
    "        self.model = FedAvg(self.model, [input.model for input in inputs])\n",
    "\n",
    "        self.results.append(\n",
    "            [\n",
    "                self.current_round,\n",
    "                self.aggregated_model_accuracy,\n",
    "                self.average_loss,\n",
    "                self.local_model_accuracy,\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        self.current_round += 1\n",
    "        if self.current_round < self.rounds:\n",
    "            self.next( self.aggregated_model_validation, foreach=\"collaborators\")\n",
    "        else:\n",
    "            self.next(self.end)\n",
    "\n",
    "    @aggregator\n",
    "    def end(self):\n",
    "        \"\"\"\n",
    "        This is the last step in the Flow.\n",
    "        \"\"\"\n",
    "        print(f\"This is the end of the flow\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0757812",
   "metadata": {},
   "source": [
    "### Simulation: LocalRuntime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bccffd7",
   "metadata": {},
   "source": [
    "We now import & define the `LocalRuntime`, participants (`Aggregator/Collaborator`), and initialize the private attributes for participants\n",
    "\n",
    "- `Runtime` – Defines where the flow runs. `LocalRuntime` simulates the flow on local node.\n",
    "- `Aggregator/Collaborator` - (Local) Participants in the simulation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bffcc141",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "from openfl.experimental.workflow.interface import Aggregator, Collaborator\n",
    "from openfl.experimental.workflow.runtime import LocalRuntime\n",
    "\n",
    "# Setup Aggregator & initialize private attributes\n",
    "aggregator = Aggregator()\n",
    "aggregator.private_attributes = {}\n",
    "\n",
    "# Setup Collaborators & initialize shards of MNIST dataset as private attributes\n",
    "n_collaborators = 2\n",
    "collaborator_names = [\"Portland\", \"Seattle\"]\n",
    "\n",
    "collaborators = [Collaborator(name=name) for name in collaborator_names]\n",
    "for idx, collaborator in enumerate(collaborators):\n",
    "    local_train = deepcopy(mnist_train)\n",
    "    local_test = deepcopy(mnist_test)\n",
    "    local_train.data = mnist_train.data[idx::n_collaborators]\n",
    "    local_train.targets = mnist_train.targets[idx::n_collaborators]\n",
    "    local_test.data = mnist_test.data[idx::n_collaborators]\n",
    "    local_test.targets = mnist_test.targets[idx::n_collaborators]\n",
    "\n",
    "    collaborator.private_attributes = {\n",
    "        \"train_loader\": torch.utils.data.DataLoader(\n",
    "            local_train, batch_size=batch_size, shuffle=False\n",
    "        ),\n",
    "        \"test_loader\": torch.utils.data.DataLoader(\n",
    "            local_test, batch_size=batch_size, shuffle=False\n",
    "        ),\n",
    "    }\n",
    "\n",
    "local_runtime = LocalRuntime(\n",
    "    aggregator=aggregator, collaborators=collaborators, backend=\"single_process\"\n",
    ")\n",
    "print(f\"Local runtime collaborators = {local_runtime.collaborators}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78819357",
   "metadata": {},
   "source": [
    "### Start Simulation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a2675ba",
   "metadata": {},
   "source": [
    "Now that we have our flow and runtime defined, let's run the simulation ! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5f10d5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "model = None\n",
    "optimizer = None\n",
    "flflow = FederatedFlow_TorchMNIST(model, optimizer, learning_rate, momentum, rounds=2, checkpoint=True)\n",
    "flflow.runtime = local_runtime\n",
    "flflow.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50300fed",
   "metadata": {},
   "source": [
    "Let us check the simulation results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5d77540",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tabulate import tabulate \n",
    "\n",
    "headers = [\"Rounds\", \"Agg Model Validation Score\", \"Local Train loss\", \"Local Model Validation score\"]\n",
    "print('********** Simulation results **********')\n",
    "simulation_results = flflow.results\n",
    "print(tabulate(simulation_results, headers=headers, tablefmt=\"outline\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5371b6d",
   "metadata": {},
   "source": [
    "### Setup Federation: Director & Envoys"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f270e385",
   "metadata": {},
   "source": [
    "Before we can deploy the experiment, let us create participants in Federation: Director and Envoys. As the Tutorial uses two collaborators we shall launch three participants:\n",
    "1. Director: The central node in the Federation\n",
    "2. Portland: The first envoy in the Federation\n",
    "3. Seattle: The second envoy in the Federation \n",
    "\n",
    "The participants can be launched by following steps mentioned in [README]((https://github.com/securefederatedai/openfl/blob/develop/openfl-tutorials/experimental/workflow/FederatedRuntime/101_MNIST/README.md))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9d556d0",
   "metadata": {},
   "source": [
    "### Deploy: FederatedRuntime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ffd73b6",
   "metadata": {},
   "source": [
    "We now import and instantiate `FederatedRuntime` to enable deployment of experiment on distributed infrastructure. Initializing the `FederatedRuntime` requires following inputs to be provided by the user:\n",
    "\n",
    "- `director_info` – director information including fqdn of the director node, port, and certificate information\n",
    "- `collaborators` - names of the collaborators participating in experiment\n",
    "- `notebook_path`- path to this jupyter notebook\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1715a373",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "from openfl.experimental.workflow.runtime import FederatedRuntime\n",
    "\n",
    "director_info = {\n",
    "    'director_node_fqdn':'localhost',\n",
    "    'director_port':50050,\n",
    "}\n",
    "\n",
    "federated_runtime = FederatedRuntime(\n",
    "    collaborators=collaborator_names,\n",
    "    director=director_info, \n",
    "    notebook_path='./101_MNIST_FederatedRuntime.ipynb'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58d22bbb",
   "metadata": {},
   "source": [
    "Let us connect to federation & check if the envoys are connected to the director by using the `get_envoys` method of `FederatedRuntime`. If the participants are launched successful in previous step the status of `Portland` and `Seattle` should be displayed as `Online`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f1be87f",
   "metadata": {},
   "outputs": [],
   "source": [
    "federated_runtime.get_envoys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87c487cb",
   "metadata": {},
   "source": [
    "Now that we have our distributed infrastructure ready, let us modify the flow runtime to `FederatedRuntime` instance and deploy the experiment. \n",
    "\n",
    "Progress of the flow is available on \n",
    "1. Jupyter notebook: if `checkpoint` attribute of the flow object is set to `True`\n",
    "2. Director and Envoy terminals  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6d19819",
   "metadata": {},
   "outputs": [],
   "source": [
    "flflow.results = [] # clear results from previous run\n",
    "flflow.runtime = federated_runtime\n",
    "flflow.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e5ef3ea",
   "metadata": {},
   "source": [
    "Let us compare the simulation results from `LocalRuntime` and federation results from `FederatedRuntime`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4b63ce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = [\"Rounds\", \"Agg Model Validation Score\", \"Local Train loss\", \"Local Model Validation score\"]\n",
    "print('********** Simulation results **********')\n",
    "print(tabulate(simulation_results, headers=headers, tablefmt=\"outline\"))\n",
    "\n",
    "print('********** Federation results **********')\n",
    "federation_results = flflow.results\n",
    "print(tabulate(federation_results, headers=headers, tablefmt=\"outline\"))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fed_run",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
