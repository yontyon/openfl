{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "dc13070c",
   "metadata": {},
   "source": [
    "# Federated Runtime: 301_MNIST_Watermarking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b7357ef",
   "metadata": {},
   "source": [
    "This tutorial is based on the LocalRuntime example [301_MNIST_Watermarking](https://github.com/securefederatedai/openfl/blob/develop/openfl-tutorials/experimental/workflow/301_MNIST_Watermarking.ipynb). It has been adapted to demonstrate the FederatedRuntime version of the watermarking workflow. In this tutorial, we will guide you through the process of deploying the watermarking example within a federation, showcasing how to transition from a local setup to a federated environment effectively."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a4394089",
   "metadata": {},
   "source": [
    "# Getting Started"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "857f9995",
   "metadata": {},
   "source": [
    "Initially, we start by specifying the module where cells marked with the `#| export` directive will be automatically exported. \n",
    "\n",
    "In the following cell, `#| default_exp experiment `indicates that the exported file will be named 'experiment'. This name can be modified based on user's requirement & preferences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d79eacbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62449b5f",
   "metadata": {},
   "source": [
    "Once we have specified the name of the module, subsequent cells of the notebook need to be *appended* by the `#| export` directive as shown below. User should ensure that *all* the notebook functionality required in the Federated Learning experiment is included in this directive"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e19dcf2",
   "metadata": {},
   "source": [
    "We start by installing OpenFL and dependencies of the workflow interface \n",
    "> These dependencies are required to be exported and become the requirements for the Federated Learning Workspace "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7475cba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "!pip install git+https://github.com/securefederatedai/openfl.git\n",
    "!pip install -r ../../../workflow_interface_requirements.txt\n",
    "!pip install matplotlib\n",
    "!pip install torch==2.3.1\n",
    "!pip install torchvision==0.18.1\n",
    "!pip install git+https://github.com/pyviz-topics/imagen.git@master\n",
    "!pip install holoviews==1.15.4\n",
    "!pip install -U ipywidgets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a6ae8e2",
   "metadata": {},
   "source": [
    "We now define our model, optimizer, and some helper functions like we would for any other deep learning experiment \n",
    "\n",
    "> This cell and all the subsequent cells are important ingredients of the Federated Learning experiment and therefore annotated with the `#| export` directive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bd8ac2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "random_seed = 1\n",
    "torch.backends.cudnn.enabled = False\n",
    "torch.manual_seed(random_seed)\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, dropout=0.0):\n",
    "        super(Net, self).__init__()\n",
    "        self.dropout = dropout\n",
    "        self.block = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, 2),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, 2),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 128, 2),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.fc1 = nn.Linear(128 * 5**2, 200)\n",
    "        self.fc2 = nn.Linear(200, 10)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.dropout(x)\n",
    "        out = self.block(x)\n",
    "        out = out.view(-1, 128 * 5**2)\n",
    "        out = self.dropout(out)\n",
    "        out = self.relu(self.fc1(out))\n",
    "        out = self.dropout(out)\n",
    "        out = self.fc2(out)\n",
    "        return F.log_softmax(out, 1)\n",
    "\n",
    "\n",
    "def inference(network, test_loader):\n",
    "    network.eval()\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            output = network(data)\n",
    "            pred = output.data.max(1, keepdim=True)[1]\n",
    "            correct += pred.eq(target.data.view_as(pred)).sum()\n",
    "    accuracy = float(correct / len(test_loader.dataset))\n",
    "    return accuracy\n",
    "\n",
    "\n",
    "def train_model(model, optimizer, data_loader, entity, round_number, log=False):\n",
    "    # Helper function to train the model\n",
    "    train_loss = 0\n",
    "    log_interval = 20\n",
    "    model.train()\n",
    "    for batch_idx, (X, y) in enumerate(data_loader):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        output = model(X)\n",
    "        loss = F.nll_loss(output, y)\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item() * len(X)\n",
    "        if batch_idx % log_interval == 0 and log:\n",
    "            print(\"{:<20} Train Epoch: {:<3} [{:<3}/{:<4} ({:<.0f}%)] Loss: {:<.6f}\".format(\n",
    "                    entity,\n",
    "                    round_number,\n",
    "                    batch_idx * len(X),\n",
    "                    len(data_loader.dataset),\n",
    "                    100.0 * batch_idx / len(data_loader),\n",
    "                    loss.item(),\n",
    "                )\n",
    "            )\n",
    "    train_loss /= len(data_loader.dataset)\n",
    "    return train_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0849d57",
   "metadata": {},
   "source": [
    "Next we import the `FLSpec` & placement decorators (`aggregator/collaborator`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89cf4866",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "from openfl.experimental.workflow.interface import FLSpec\n",
    "from openfl.experimental.workflow.placement import aggregator, collaborator\n",
    "\n",
    "def FedAvg(agg_model, models, weights=None):\n",
    "    state_dicts = [model.state_dict() for model in models]\n",
    "    state_dict = agg_model.state_dict()\n",
    "    for key in models[0].state_dict():\n",
    "        state_dict[key] = torch.from_numpy(np.average([state[key].numpy() for state in state_dicts],\n",
    "                                                      axis=0, \n",
    "                                                      weights=weights))\n",
    "        \n",
    "    agg_model.load_state_dict(state_dict)\n",
    "    return agg_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36ed5e31",
   "metadata": {},
   "source": [
    "Let us now define the Workflow for Watermark embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52c4a752",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "class FederatedFlow_MNIST_Watermarking(FLSpec):\n",
    "    \"\"\"\n",
    "    This Flow demonstrates Watermarking on a Deep Learning Model in Federated Learning\n",
    "    Ref: WAFFLE: Watermarking in Federated Learning (https://arxiv.org/abs/2008.07298)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        model=None,\n",
    "        optimizer=None,\n",
    "        watermark_pretrain_optimizer=None,\n",
    "        watermark_retrain_optimizer=None,\n",
    "        round_number=0,\n",
    "        n_rounds=1,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "        if model is not None:\n",
    "            self.model = model\n",
    "            self.optimizer = optimizer\n",
    "            self.watermark_pretrain_optimizer = watermark_pretrain_optimizer\n",
    "            self.watermark_retrain_optimizer = watermark_retrain_optimizer\n",
    "        else:\n",
    "            self.model = Net()\n",
    "            self.optimizer = optim.SGD(\n",
    "                self.model.parameters(), lr=learning_rate, momentum=momentum\n",
    "            )\n",
    "            self.watermark_pretrain_optimizer = optim.SGD(\n",
    "                self.model.parameters(),\n",
    "                lr=watermark_pretrain_learning_rate,\n",
    "                momentum=watermark_pretrain_momentum,\n",
    "                weight_decay=watermark_pretrain_weight_decay,\n",
    "            )\n",
    "            self.watermark_retrain_optimizer = optim.SGD(\n",
    "                self.model.parameters(), lr=watermark_retrain_learning_rate\n",
    "            )\n",
    "        self.round_number = round_number\n",
    "        self.n_rounds = n_rounds\n",
    "        self.watermark_pretraining_completed = False\n",
    "\n",
    "    @aggregator\n",
    "    def start(self):\n",
    "        \"\"\"\n",
    "        This is the start of the Flow.\n",
    "        \"\"\"\n",
    "        print(\"<Agg>: Start of flow ... \")\n",
    "        self.collaborators = self.runtime.collaborators\n",
    "\n",
    "        self.next(self.watermark_pretrain)\n",
    "\n",
    "    @aggregator\n",
    "    def watermark_pretrain(self):\n",
    "        \"\"\"\n",
    "        Pre-Train the Model before starting Federated Learning.\n",
    "        \"\"\"\n",
    "        if not self.watermark_pretraining_completed:\n",
    "\n",
    "            print(\"<Agg>: Performing Watermark Pre-training\")\n",
    "\n",
    "            for i in range(self.pretrain_epochs):\n",
    "\n",
    "                watermark_pretrain_loss = train_model(\n",
    "                    self.model,\n",
    "                    self.watermark_pretrain_optimizer,\n",
    "                    self.watermark_data_loader,\n",
    "                    \"<Agg>:\",\n",
    "                    i,\n",
    "                    log=False,\n",
    "                )\n",
    "                watermark_pretrain_validation_score = inference(\n",
    "                    self.model, self.watermark_data_loader\n",
    "                )\n",
    "\n",
    "                print(f\"<Agg>: Watermark Pretraining: Round: {i:<3}\"\n",
    "                      + f\" Loss: {watermark_pretrain_loss:<.6f}\"\n",
    "                      + f\" Acc: {watermark_pretrain_validation_score:<.6f}\")\n",
    "\n",
    "            self.watermark_pretraining_completed = True\n",
    "\n",
    "        self.next(\n",
    "            self.aggregated_model_validation,\n",
    "            foreach=\"collaborators\",\n",
    "        )\n",
    "\n",
    "    @collaborator\n",
    "    def aggregated_model_validation(self):\n",
    "        \"\"\"\n",
    "        Perform Aggregated Model validation on Collaborators.\n",
    "        \"\"\"\n",
    "        self.agg_validation_score = inference(self.model, self.test_loader)\n",
    "        print(f\"<Collab: {self.input}>\"\n",
    "              + f\" Aggregated Model validation score = {self.agg_validation_score}\"\n",
    "              )\n",
    "\n",
    "        self.next(self.train)\n",
    "\n",
    "    @collaborator\n",
    "    def train(self):\n",
    "        \"\"\"\n",
    "        Train model on Local collab dataset.\n",
    "        \"\"\"\n",
    "        print(\"<Collab>: Performing Model Training on Local dataset ... \")\n",
    "\n",
    "        self.optimizer = optim.SGD(\n",
    "            self.model.parameters(), lr=learning_rate, momentum=momentum\n",
    "        )\n",
    "\n",
    "        self.loss = train_model(\n",
    "            self.model,\n",
    "            self.optimizer,\n",
    "            self.train_loader,\n",
    "            f\"<Collab: {self.input}>\",\n",
    "            self.round_number,\n",
    "            log=True,\n",
    "        )\n",
    "\n",
    "        self.next(self.local_model_validation)\n",
    "\n",
    "    @collaborator\n",
    "    def local_model_validation(self):\n",
    "        \"\"\"\n",
    "        Validate locally trained model.\n",
    "        \"\"\"\n",
    "        self.local_validation_score = inference(self.model, self.test_loader)\n",
    "        print(\n",
    "            f\"<Collab: {self.input}> Local model validation score = {self.local_validation_score}\"\n",
    "        )\n",
    "        self.next(self.join)\n",
    "\n",
    "    @aggregator\n",
    "    def join(self, inputs):\n",
    "        \"\"\"\n",
    "        Model aggregation step.\n",
    "        \"\"\"\n",
    "        self.average_loss = sum(input.loss for input in inputs) / len(inputs)\n",
    "        self.aggregated_model_accuracy = sum(\n",
    "            input.agg_validation_score for input in inputs\n",
    "        ) / len(inputs)\n",
    "        self.local_model_accuracy = sum(\n",
    "            input.local_validation_score for input in inputs\n",
    "        ) / len(inputs)\n",
    "\n",
    "        print(\"<Agg>: Joining models from collaborators...\")\n",
    "\n",
    "        print(\n",
    "            f\"   Aggregated model validation score = {self.aggregated_model_accuracy}\"\n",
    "        )\n",
    "        print(f\"   Average training loss = {self.average_loss}\")\n",
    "        print(f\"   Average local model validation values = {self.local_model_accuracy}\")\n",
    "\n",
    "        self.model = FedAvg(self.model, [input.model for input in inputs])\n",
    "\n",
    "        self.next(self.watermark_retrain)\n",
    "\n",
    "    @aggregator\n",
    "    def watermark_retrain(self):\n",
    "        \"\"\"\n",
    "        Retrain the aggregated model.\n",
    "        \"\"\"\n",
    "        print(\"<Agg>: Performing Watermark Retraining ... \")\n",
    "        self.watermark_retrain_optimizer = optim.SGD(\n",
    "            self.model.parameters(), lr=watermark_retrain_learning_rate\n",
    "        )\n",
    "\n",
    "        retrain_round = 0\n",
    "\n",
    "        # Perform re-training until (accuracy >= acc_threshold) or\n",
    "        # (retrain_round > number of retrain_epochs)\n",
    "        self.watermark_retrain_validation_score = inference(\n",
    "            self.model, self.watermark_data_loader\n",
    "        )\n",
    "        while (\n",
    "            self.watermark_retrain_validation_score < self.watermark_acc_threshold\n",
    "        ) and (retrain_round < self.retrain_epochs):\n",
    "            self.watermark_retrain_train_loss = train_model(\n",
    "                self.model,\n",
    "                self.watermark_retrain_optimizer,\n",
    "                self.watermark_data_loader,\n",
    "                \"<Agg>\",\n",
    "                retrain_round,\n",
    "                log=False,\n",
    "            )\n",
    "            self.watermark_retrain_validation_score = inference(\n",
    "                self.model, self.watermark_data_loader\n",
    "            )\n",
    "\n",
    "            print(f\"<Agg>: Watermark Retraining: Train Epoch: {self.round_number:<3}\"\n",
    "                  + f\" Retrain Round: {retrain_round:<3}\"\n",
    "                  + f\" Loss: {self.watermark_retrain_train_loss:<.6f},\"\n",
    "                  + f\" Acc: {self.watermark_retrain_validation_score:<.6f}\")\n",
    "            retrain_round += 1\n",
    "\n",
    "        self.next(self.end)\n",
    "\n",
    "    @aggregator\n",
    "    def end(self):\n",
    "        \"\"\"\n",
    "        This is the last step in the Flow.\n",
    "        \"\"\"\n",
    "        print(\"This is the end of the flow\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5371b6d",
   "metadata": {},
   "source": [
    "## Defining and Initializing the Federated Runtime\n",
    "We initialize the Federated Runtime by providing:\n",
    "- `director_info`: The director's connection information \n",
    "- `authorized_collaborators`: A list of authorized collaborators\n",
    "- `notebook_path`: Path to this Jupyter notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1715a373",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "from openfl.experimental.workflow.runtime import FederatedRuntime\n",
    "\n",
    "director_info = {\n",
    "    'director_node_fqdn':'localhost',\n",
    "    'director_port':50050,\n",
    "}\n",
    "\n",
    "authorized_collaborators = ['Bangalore', 'Chandler']\n",
    "\n",
    "federated_runtime = FederatedRuntime(\n",
    "    collaborators=authorized_collaborators,\n",
    "    director=director_info, \n",
    "    notebook_path='./MNIST_Watermarking.ipynb'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6de9684f",
   "metadata": {},
   "source": [
    "The status of the connected Envoys can be checked using the `get_envoys()` method of the `federated_runtime`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f1be87f",
   "metadata": {},
   "outputs": [],
   "source": [
    "federated_runtime.get_envoys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eaeca25",
   "metadata": {},
   "source": [
    "With the federated_runtime now instantiated, we will proceed to deploy the watermarking workspace and run the experiment!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6d19819",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "# Set random seed\n",
    "random_seed = 42\n",
    "torch.manual_seed(random_seed)\n",
    "np.random.seed(random_seed)\n",
    "torch.backends.cudnn.enabled = False\n",
    "\n",
    "# MNIST parameters\n",
    "learning_rate = 5e-2\n",
    "momentum = 5e-1\n",
    "log_interval = 20\n",
    "\n",
    "# Watermarking parameters\n",
    "watermark_pretrain_learning_rate = 1e-1\n",
    "watermark_pretrain_momentum = 5e-1\n",
    "watermark_pretrain_weight_decay = 5e-05\n",
    "watermark_retrain_learning_rate = 5e-3\n",
    "\n",
    "model = Net()\n",
    "optimizer = optim.SGD(\n",
    "    model.parameters(), lr=learning_rate, momentum=momentum\n",
    ")\n",
    "watermark_pretrain_optimizer = optim.SGD(\n",
    "    model.parameters(),\n",
    "    lr=watermark_pretrain_learning_rate,\n",
    "    momentum=watermark_pretrain_momentum,\n",
    "    weight_decay=watermark_pretrain_weight_decay,\n",
    ")\n",
    "watermark_retrain_optimizer = optim.SGD(\n",
    "    model.parameters(), lr=watermark_retrain_learning_rate\n",
    ")\n",
    "\n",
    "flflow = FederatedFlow_MNIST_Watermarking(\n",
    "    model,\n",
    "    optimizer,\n",
    "    watermark_pretrain_optimizer,\n",
    "    watermark_retrain_optimizer,\n",
    "    checkpoint=True,\n",
    ")\n",
    "flflow.runtime = federated_runtime\n",
    "flflow.run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fed_run",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
